name: Scheduled Crawler

on:
  schedule:
    - cron: "55 3 * * *" # 每天UTC 4點 (台北12點)
  workflow_dispatch:
  
jobs:
  crawler:
    runs-on: ubuntu-latest
          
    steps:
      - name: Install Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose
          
      # Step 1: 檢出程式碼
      - name: Checkout code
        uses: actions/checkout@v3

      # Step 2: 設定 Docker Compose
      - name: Set up Docker Compose
        run: |
          docker volume create --name=mysql
          docker network create my_network
          docker-compose -f mysql.yml up -d

      - name: List running Docker containers
        run: |
          docker ps
          docker network ls

      - name: 等待 MySQL 啟動
        run: |
          for i in {1..10}; do
            docker exec linpoi387_mysql_1 mysqladmin ping -h "127.0.0.1" -u root -ptest && break
            echo "等待 MySQL 啟動..."
            sleep 3
          done
          
      - name: Import SQL
        run: |
          docker exec -i linpoi387_mysql_1 mysql -h 127.0.0.1 -u root -p${{ secrets.MYSQL_ADMIN_PASSWORD }} crawler < ./crawler.sql

      # Step 5: Display phpMyAdmin URL     
      - name: Install ngrok
        run: |
          curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null
          echo "deb https://ngrok-agent.s3.amazonaws.com buster main" | sudo tee /etc/apt/sources.list.d/ngrok.list
          sudo apt update
          sudo apt install -y ngrok
          
      - name: Add ngrok authtoken
        run: ngrok config add-authtoken ${{ secrets.NGROK_AUTHTOKEN }}
        
      - name: Expose phpMyAdmin to the internet
        run: ngrok http 8000 &

      - name: pip install
        run: |
          pip install apscheduler
          pip install loguru
          pip install requests
          pip install beautifulsoup4
          pip install mysql.connector
          pip install fastapi
          pip install uvicorn
          pip install sqlalchemy
          pip install pandas
          
      # Step 4: 執行爬蟲
      - name: api_scheduler.py
        run: python api_scheduler.py   

      # Save the file as an artifact (replace the old one)
      - name: Export SQL
        run: |
          docker exec -i linpoi387_mysql_1 mysqldump -h 127.0.0.1 -u root -p${{ secrets.MYSQL_ADMIN_PASSWORD }} crawler > ./crawler.sql
          
      - name: Upload Artifact
        uses: actions/upload-artifact@v3
        with:
          name: crawler.sql
          path: ./crawler.sql    
          
      # Step 5: 停止 Docker Compose
      - name: Stop Docker Compose
        run: |
          docker-compose -f mysql.yml down
